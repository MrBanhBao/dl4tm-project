{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, sentences, next_words, maxlen, word_index, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.next_words = next_words\n",
    "        self.sentences = sentences\n",
    "        self.shuffle = shuffle\n",
    "        self.maxlen = maxlen\n",
    "        self.word_index = word_index\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        sentences_temp = [self.sentences[k] for k in indexes]\n",
    "        next_words_temp = [self.next_words[l] for l in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(sentences_temp, next_words_temp)\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, sentences, next_words):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((len(sentences), self.maxlen, len(word_index))) # (sentences)\n",
    "        y = np.zeros((len(sentences), len(word_index)), dtype=np.bool)\n",
    "\n",
    "        # Generate data\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, word in enumerate(sentence):\n",
    "                X[i, t, word_index[word]] = 1    # one hot encoding\n",
    "                y[i, word_index[next_words[i]]] = 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def normalize_lyric(text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub('\\[.+\\](\\\\n)|\\[.+\\](\\(.*\\))', '', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../data/deutsch'\n",
    "artists = ['Bushido']\n",
    "\n",
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "lyrics = df.lyrics.map(lambda lyric: normalize_lyric(lyric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 58768\n"
     ]
    }
   ],
   "source": [
    "lyrics_in_words = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.replace('\\n', ' \\n ').lower()\n",
    "    words = lyric.split(' ')\n",
    "    lyrics_in_words.append(words)\n",
    "    \n",
    "print('Corpus length in words:', len(list(itertools.chain(*lyrics_in_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 9013\n"
     ]
    }
   ],
   "source": [
    "words = set(list(itertools.chain(*lyrics_in_words)))\n",
    "print('Unique words:', len(words))\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "index_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 7\n",
    "step = 1\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "for lyric in lyrics_in_words:\n",
    "    for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "        sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "        next_words.append(lyric[i + maxlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datagenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, next_words_train, next_words_test = train_test_split(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "DIR = '../outputs/wordbased/LSTM_Simple_WordBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS, BATCH_SIZE, maxlen, step)\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3c3be79dfb2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator(sentences_train, next_words_train, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "test_generator = DataGenerator(sentences_test, next_words_test, maxlen, word_index, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)\n",
    "#earlyStopping = EarlyStopping(patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(words))))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               4680704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9013)              1162677   \n",
      "=================================================================\n",
      "Total params: 5,843,381\n",
      "Trainable params: 5,843,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "680/680 [==============================] - 148s 218ms/step - loss: 6.4584 - val_loss: 6.0204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129cca438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "680/680 [==============================] - 93s 137ms/step - loss: 6.4056 - val_loss: 6.0075\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 110s 161ms/step - loss: 5.7705 - val_loss: 6.3474\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 5.8172 - val_loss: 6.4465\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 107s 158ms/step - loss: 5.4178 - val_loss: 6.3681\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 4.9444 - val_loss: 6.4031\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 109s 160ms/step - loss: 4.5167 - val_loss: 6.3972\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 114s 167ms/step - loss: 4.2346 - val_loss: 6.4783\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 94s 138ms/step - loss: 4.1270 - val_loss: 6.4382\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 63s 92ms/step - loss: 4.0789 - val_loss: 6.5317\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 103s 152ms/step - loss: 4.0261 - val_loss: 6.4989\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 107s 157ms/step - loss: 3.9873 - val_loss: 6.4894\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 104s 152ms/step - loss: 3.9297 - val_loss: 6.5882\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 107s 157ms/step - loss: 3.9026 - val_loss: 6.5327\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 111s 163ms/step - loss: 3.8666 - val_loss: 6.5428\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 102s 150ms/step - loss: 3.7909 - val_loss: 6.5543\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 112s 164ms/step - loss: 3.7462 - val_loss: 6.5926\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 111s 163ms/step - loss: 3.7337 - val_loss: 6.6660\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 109s 161ms/step - loss: 3.7164 - val_loss: 6.5770\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 112s 165ms/step - loss: 3.6878 - val_loss: 6.6148\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 106s 155ms/step - loss: 3.6446 - val_loss: 6.6444\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.6195 - val_loss: 6.6442\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.6004 - val_loss: 6.6228\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.5627 - val_loss: 6.5894\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 109s 161ms/step - loss: 3.5570 - val_loss: 6.6776\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 114s 167ms/step - loss: 3.5340 - val_loss: 6.7228\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 88s 129ms/step - loss: 3.5165 - val_loss: 6.6460\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 111s 164ms/step - loss: 3.5098 - val_loss: 6.6398\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 113s 167ms/step - loss: 3.5027 - val_loss: 6.6281\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 109s 160ms/step - loss: 3.4938 - val_loss: 6.6045\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 106s 156ms/step - loss: 3.4952 - val_loss: 6.6378\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.5049 - val_loss: 6.5854\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 107s 158ms/step - loss: 3.5389 - val_loss: 6.5566\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 107s 157ms/step - loss: 3.5410 - val_loss: 6.5048\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 109s 160ms/step - loss: 3.5409 - val_loss: 6.5779\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 109s 161ms/step - loss: 3.5535 - val_loss: 6.5658\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 113s 166ms/step - loss: 3.5527 - val_loss: 6.4952\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 107s 157ms/step - loss: 3.5402 - val_loss: 6.4918\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 112s 165ms/step - loss: 3.5360 - val_loss: 6.5941\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 109s 160ms/step - loss: 3.5248 - val_loss: 6.6219\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 108s 158ms/step - loss: 3.5189 - val_loss: 6.5817\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 112s 165ms/step - loss: 3.5108 - val_loss: 6.6318\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 112s 164ms/step - loss: 3.5053 - val_loss: 6.5661\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 111s 164ms/step - loss: 3.4924 - val_loss: 6.5537\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.4873 - val_loss: 6.5450\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.4721 - val_loss: 6.5761\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 108s 160ms/step - loss: 3.4632 - val_loss: 6.5959\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 105s 155ms/step - loss: 3.4617 - val_loss: 6.5982\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.4457 - val_loss: 6.6600\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 113s 166ms/step - loss: 3.4521 - val_loss: 6.5969\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 110s 161ms/step - loss: 3.4440 - val_loss: 6.5837\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 99s 146ms/step - loss: 3.4268 - val_loss: 6.5539\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 109s 161ms/step - loss: 3.4170 - val_loss: 6.6081\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 111s 164ms/step - loss: 3.4183 - val_loss: 6.6531\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 105s 155ms/step - loss: 3.4161 - val_loss: 6.6342\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 107s 157ms/step - loss: 3.4017 - val_loss: 6.5854\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 110s 161ms/step - loss: 3.4072 - val_loss: 6.6729\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 106s 157ms/step - loss: 3.4018 - val_loss: 6.5848\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.3887 - val_loss: 6.6115\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 110s 162ms/step - loss: 3.3885 - val_loss: 6.6583\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 111s 164ms/step - loss: 3.3872 - val_loss: 6.7149\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 112s 164ms/step - loss: 3.3792 - val_loss: 6.7010\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 112s 165ms/step - loss: 3.3809 - val_loss: 6.6121\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 108s 159ms/step - loss: 3.3815 - val_loss: 6.6316\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 113s 166ms/step - loss: 3.3819 - val_loss: 6.6113\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 107s 158ms/step - loss: 3.3761 - val_loss: 6.5765\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 109s 161ms/step - loss: 3.3764 - val_loss: 6.6003\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 111s 163ms/step - loss: 3.3706 - val_loss: 6.5612\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 111s 163ms/step - loss: 3.3657 - val_loss: 6.6916\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 109s 160ms/step - loss: 3.3569 - val_loss: 6.6614\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 105s 154ms/step - loss: 3.3558 - val_loss: 6.6462\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 108s 159ms/step - loss: 3.3447 - val_loss: 6.6045\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 90s 132ms/step - loss: 3.3486 - val_loss: 6.6794\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 50s 74ms/step - loss: 3.3474 - val_loss: 6.6789\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 52s 77ms/step - loss: 3.3384 - val_loss: 6.7161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25\n",
      "680/680 [==============================] - 51s 75ms/step - loss: 3.3374 - val_loss: 6.6883\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 35s 52ms/step - loss: 3.3359 - val_loss: 6.6697\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 49s 72ms/step - loss: 3.3337 - val_loss: 6.6148\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 49s 72ms/step - loss: 3.3333 - val_loss: 6.6808\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 51s 75ms/step - loss: 3.3307 - val_loss: 6.6324\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 51s 75ms/step - loss: 3.3280 - val_loss: 6.5865\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 51s 75ms/step - loss: 3.3287 - val_loss: 6.6537\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 47s 70ms/step - loss: 3.3263 - val_loss: 6.6339\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 50s 73ms/step - loss: 3.3263 - val_loss: 6.6411\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 51s 74ms/step - loss: 3.3253 - val_loss: 6.5770\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 52s 77ms/step - loss: 3.3231 - val_loss: 6.6132\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 51s 74ms/step - loss: 3.3285 - val_loss: 6.6050\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 49s 73ms/step - loss: 3.3287 - val_loss: 6.6065\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 51s 74ms/step - loss: 3.3258 - val_loss: 6.5713\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 53s 78ms/step - loss: 3.3198 - val_loss: 6.5970\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 51s 75ms/step - loss: 3.3206 - val_loss: 6.6361\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 49s 72ms/step - loss: 3.3182 - val_loss: 6.5805\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 50s 73ms/step - loss: 3.3146 - val_loss: 6.6261\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 50s 74ms/step - loss: 3.3180 - val_loss: 6.5995\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 47s 69ms/step - loss: 3.3059 - val_loss: 6.6012\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3030 - val_loss: 6.6242\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2985 - val_loss: 6.6076\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2996 - val_loss: 6.5482\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3077 - val_loss: 6.6075\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3098 - val_loss: 6.6093\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3050 - val_loss: 6.6006\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 28s 40ms/step - loss: 3.2996 - val_loss: 6.6189\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.2964 - val_loss: 6.6842\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.3030 - val_loss: 6.6530\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2971 - val_loss: 6.6190\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3097 - val_loss: 6.6144\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.3075 - val_loss: 6.6359\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.3046 - val_loss: 6.5999\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.3061 - val_loss: 6.6361\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2985 - val_loss: 6.6429\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2992 - val_loss: 6.6139\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.3011 - val_loss: 6.6273\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.3168 - val_loss: 6.5901\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 38s 57ms/step - loss: 3.3079 - val_loss: 6.5914\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.2988 - val_loss: 6.6351\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3005 - val_loss: 6.6027\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2950 - val_loss: 6.6075\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2910 - val_loss: 6.6001\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 38s 55ms/step - loss: 3.2910 - val_loss: 6.6437\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 38s 56ms/step - loss: 3.2993 - val_loss: 6.5972\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3003 - val_loss: 6.6128\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2992 - val_loss: 6.6681\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2892 - val_loss: 6.6080\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2928 - val_loss: 6.6812\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2935 - val_loss: 6.6656\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2857 - val_loss: 6.6111\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 28s 41ms/step - loss: 3.2948 - val_loss: 6.6754\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.2854 - val_loss: 6.6540\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2895 - val_loss: 6.5906\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 38s 57ms/step - loss: 3.3021 - val_loss: 6.6419\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.3088 - val_loss: 6.6014\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2906 - val_loss: 6.6157\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2915 - val_loss: 6.5503\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2936 - val_loss: 6.6454\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2986 - val_loss: 6.6325\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2979 - val_loss: 6.5984\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.3007 - val_loss: 6.5901\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2778 - val_loss: 6.6398\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2967 - val_loss: 6.5815\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2925 - val_loss: 6.6498\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 37s 55ms/step - loss: 3.2938 - val_loss: 6.5899\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2918 - val_loss: 6.6227\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2827 - val_loss: 6.6308\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2879 - val_loss: 6.6575\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2855 - val_loss: 6.5813\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 38s 56ms/step - loss: 3.2737 - val_loss: 6.6165\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2809 - val_loss: 6.6178\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2812 - val_loss: 6.6060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25\n",
      "680/680 [==============================] - 40s 59ms/step - loss: 3.2869 - val_loss: 6.5771\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2854 - val_loss: 6.6499\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 38s 56ms/step - loss: 3.2887 - val_loss: 6.6200\n",
      "wenn der benz anspringt und die reifen_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gruppe3B/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 26s 38ms/step - loss: 3.2898 - val_loss: 6.6085\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.2874 - val_loss: 6.5863\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 40s 59ms/step - loss: 3.2843 - val_loss: 6.5918\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2874 - val_loss: 6.6076\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2976 - val_loss: 6.6434\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.2687 - val_loss: 6.5953\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2786 - val_loss: 6.6186\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2747 - val_loss: 6.6556\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2783 - val_loss: 6.6118\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2848 - val_loss: 6.6162\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2787 - val_loss: 6.5775\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2740 - val_loss: 6.5872\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2852 - val_loss: 6.5808\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2929 - val_loss: 6.6284\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2886 - val_loss: 6.5743\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 38s 56ms/step - loss: 3.2972 - val_loss: 6.6526\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2955 - val_loss: 6.5995\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 40s 58ms/step - loss: 3.2833 - val_loss: 6.6150\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2767 - val_loss: 6.6451\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2776 - val_loss: 6.6046\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2736 - val_loss: 6.6015\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2797 - val_loss: 6.6500\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2863 - val_loss: 6.6076\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2877 - val_loss: 6.6186\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2827 - val_loss: 6.6431\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 25s 37ms/step - loss: 3.2763 - val_loss: 6.6150\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 39s 57ms/step - loss: 3.2757 - val_loss: 6.6684\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2801 - val_loss: 6.6166\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 39s 58ms/step - loss: 3.2711 - val_loss: 6.6506\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 38s 55ms/step - loss: 3.2893 - val_loss: 6.6679\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2795 - val_loss: 6.6914\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2769 - val_loss: 6.6129\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 36s 53ms/step - loss: 3.2700 - val_loss: 6.5730\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2849 - val_loss: 6.6480\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2794 - val_loss: 6.6339\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2772 - val_loss: 6.5789\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2802 - val_loss: 6.6434\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2817 - val_loss: 6.6123\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2831 - val_loss: 6.6328\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2870 - val_loss: 6.6653\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2820 - val_loss: 6.6434\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2803 - val_loss: 6.6491\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2895 - val_loss: 6.6682\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2804 - val_loss: 6.6288\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2852 - val_loss: 6.6096\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2830 - val_loss: 6.6383\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2792 - val_loss: 6.6377\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2666 - val_loss: 6.6359\n",
      "Epoch 24/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2659 - val_loss: 6.6032\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 36s 53ms/step - loss: 3.2899 - val_loss: 6.6239\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "Epoch 1/25\n",
      "680/680 [==============================] - 28s 40ms/step - loss: 3.2838 - val_loss: 6.5748\n",
      "Epoch 2/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2837 - val_loss: 6.6510\n",
      "Epoch 3/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2778 - val_loss: 6.5903\n",
      "Epoch 4/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2915 - val_loss: 6.5760\n",
      "Epoch 5/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2865 - val_loss: 6.6202\n",
      "Epoch 6/25\n",
      "680/680 [==============================] - 37s 55ms/step - loss: 3.2861 - val_loss: 6.6161\n",
      "Epoch 7/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2838 - val_loss: 6.6417\n",
      "Epoch 8/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.3022 - val_loss: 6.6032\n",
      "Epoch 9/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2939 - val_loss: 6.6624\n",
      "Epoch 10/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2831 - val_loss: 6.5824\n",
      "Epoch 11/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2960 - val_loss: 6.6266\n",
      "Epoch 12/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2821 - val_loss: 6.6684\n",
      "Epoch 13/25\n",
      "680/680 [==============================] - 37s 55ms/step - loss: 3.2755 - val_loss: 6.6541\n",
      "Epoch 14/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2722 - val_loss: 6.6670\n",
      "Epoch 15/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2884 - val_loss: 6.6162\n",
      "Epoch 16/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2770 - val_loss: 6.6594\n",
      "Epoch 17/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2860 - val_loss: 6.6440\n",
      "Epoch 18/25\n",
      "680/680 [==============================] - 36s 53ms/step - loss: 3.2826 - val_loss: 6.6414\n",
      "Epoch 19/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2968 - val_loss: 6.6075\n",
      "Epoch 20/25\n",
      "680/680 [==============================] - 36s 53ms/step - loss: 3.2918 - val_loss: 6.6255\n",
      "Epoch 21/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2817 - val_loss: 6.6735\n",
      "Epoch 22/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2913 - val_loss: 6.6446\n",
      "Epoch 23/25\n",
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2896 - val_loss: 6.6242\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 37s 54ms/step - loss: 3.2811 - val_loss: 6.6487\n",
      "Epoch 25/25\n",
      "680/680 [==============================] - 36s 54ms/step - loss: 3.2905 - val_loss: 6.7051\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n"
     ]
    }
   ],
   "source": [
    "#AUTO TRAIN\n",
    "BATCH_SIZE = 64\n",
    "RANGE = 10\n",
    "GEN_WORD_LEN = 453\n",
    "training_generator = DataGenerator(sentences_train, next_words_train, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "test_generator = DataGenerator(sentences_test, next_words_test, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "for it in range(1, RANGE):\n",
    "    EPOCHS = 25\n",
    "    DIR = '../outputs/wordbased/LSTM_Simple_WordBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS*it, BATCH_SIZE, maxlen, step)\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "        \n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "    modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "    modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)\n",
    "\n",
    "    model.fit_generator(training_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])\n",
    "    \n",
    "    ##########\n",
    "    temperatures = [0.2, 0.4, 0.5, 0.6, 0.8, 1.]\n",
    "    for temperature in temperatures:\n",
    "        generated_text_temp = [\"wenn\", \"der\", \"benz\", \"anspringt\", \"und\", \"die\", \"reifen\"]\n",
    "        generated_text = copy(generated_text_temp)\n",
    "        print(\" \".join(generated_text) + '_')\n",
    "        #print('\\n___________________\\n')\n",
    "        for i in range(GEN_WORD_LEN):\n",
    "            sampled = np.zeros((1, maxlen, len(words)))\n",
    "\n",
    "            for t, word in enumerate(generated_text_temp):\n",
    "                sampled[0, t, word_index[word]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = index_word[next_index]\n",
    "            generated_text_temp.append(next_word)\n",
    "            generated_text.append(next_word)\n",
    "            generated_text_temp = generated_text_temp[1:]\n",
    "            #sys.stdout.write(\" \" + next_word)\n",
    "            \n",
    "        with open(os.path.join(DIR, '{}_temp{}_text.txt'.format(artists[0], temperature)), 'w+') as text_file:\n",
    "            text_file.write(' '.join(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wenn der benz anspringt und die reifen_\n",
      " tag \n",
      " ich bin ein - und du bist \n",
      " doch es ist noch immer wie ein mann \n",
      "  \n",
      "  \n",
      " kannst du die es sehen, kannst du gott haben mein mit hat \n",
      " in bleib mehr wenn der meine nicht ist \n",
      " mein rap ist geld und mein hund \n",
      " die es der nicht mehr einfach aus \n",
      " mein ohne ist mehr mit als weil es zum nicht mehr wie \n",
      " du für mir scheiße nie mehr von der - ich kannst du nicht mit mir auf sein \n",
      " du niemals mich einfach an der vater \n"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "random.seed(3004)\n",
    "\n",
    "#lyrics_index = random.randint(0, len(lyrics))\n",
    "#chosen_lyric = lyrics_in_words[lyrics_index]\n",
    "#start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "#generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text = [\"wenn\", \"der\", \"benz\", \"anspringt\", \"und\", \"die\", \"reifen\"]\n",
    "generated_text_temp = copy(generated_text_temp)\n",
    "print(\" \".join(generated_text) + '_')\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(100):\n",
    "    sampled = np.zeros((1, maxlen, len(words)))\n",
    "            \n",
    "    for t, word in enumerate(generated_text_temp):\n",
    "        sampled[0, t, word_index[word]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_word = index_word[next_index]\n",
    "    generated_text_temp.append(next_word)\n",
    "    generated_text.append(next_word)\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(\" \" + next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auf', 'dem', '\\n', 'ihr', '', '\\n', 'black,']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wenn',\n",
       " 'der',\n",
       " 'benz',\n",
       " 'anspringt',\n",
       " 'und',\n",
       " 'die',\n",
       " 'reifen',\n",
       " 'immer',\n",
       " 'als',\n",
       " 'ein',\n",
       " 'der',\n",
       " 'in',\n",
       " 'deinen',\n",
       " 'kein',\n",
       " 'im',\n",
       " '\\n',\n",
       " 'bist',\n",
       " 'weil',\n",
       " 'ein',\n",
       " 'mir',\n",
       " 'wenn',\n",
       " 'die',\n",
       " 'du',\n",
       " 'bist',\n",
       " 'ich',\n",
       " 'nicht',\n",
       " 'dich',\n",
       " 'a',\n",
       " 'als',\n",
       " '\\n',\n",
       " 'am',\n",
       " 'den',\n",
       " 'mir',\n",
       " 'am',\n",
       " 'was',\n",
       " 'ihr',\n",
       " 'dass',\n",
       " 'ihr',\n",
       " 'einfach',\n",
       " 'nicht',\n",
       " 'mal',\n",
       " 'und',\n",
       " 'die',\n",
       " 'dir',\n",
       " 'nicht',\n",
       " 'mehr',\n",
       " '\\n',\n",
       " 'ihr',\n",
       " 'habt',\n",
       " 'alle',\n",
       " \"'ner\",\n",
       " 'gegen',\n",
       " 'auf',\n",
       " '\\n',\n",
       " 'mann,',\n",
       " 'gibt',\n",
       " 'es',\n",
       " 'keine',\n",
       " 'wieder',\n",
       " 'war',\n",
       " '\\n',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'immer',\n",
       " 'noch',\n",
       " 'der',\n",
       " 'nicht',\n",
       " 'in',\n",
       " 'meinen',\n",
       " 'cool',\n",
       " '\\n',\n",
       " 'so',\n",
       " 'dein',\n",
       " 'ihr',\n",
       " 'leben',\n",
       " 'war',\n",
       " '\\n',\n",
       " 'ich',\n",
       " 'hab',\n",
       " \"'ne\",\n",
       " 'ein',\n",
       " 'mit',\n",
       " 'der',\n",
       " 'nie',\n",
       " 'mehr',\n",
       " 'nach',\n",
       " 'der',\n",
       " 'sein',\n",
       " '\\n',\n",
       " 'du',\n",
       " 'hast',\n",
       " 'ein',\n",
       " 'es',\n",
       " 'gibt',\n",
       " 'seine',\n",
       " \"'ne\",\n",
       " '\\n',\n",
       " 'wir',\n",
       " 'sind',\n",
       " 'ein',\n",
       " 'auf',\n",
       " 'dem',\n",
       " '\\n',\n",
       " 'ihr',\n",
       " '',\n",
       " '\\n',\n",
       " 'black,']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
