{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, sentences, next_words, maxlen, word_index, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.next_words = next_words\n",
    "        self.sentences = sentences\n",
    "        self.shuffle = shuffle\n",
    "        self.maxlen = maxlen\n",
    "        self.word_index = word_index\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        sentences_temp = [self.sentences[k] for k in indexes]\n",
    "        next_words_temp = [self.next_words[l] for l in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(sentences_temp, next_words_temp)\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, sentences, next_words):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((len(sentences), self.maxlen, len(word_index))) # (sentences)\n",
    "        y = np.zeros((len(sentences), len(word_index)), dtype=np.bool)\n",
    "\n",
    "        # Generate data\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, word in enumerate(sentence):\n",
    "                X[i, t, word_index[word]] = 1    # one hot encoding\n",
    "                y[i, word_index[next_words[i]]] = 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def normalize_lyric(text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub('\\[.+\\](\\\\n)|\\[.+\\](\\(.*\\))', '', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../data/deutsch'\n",
    "artists = ['Bushido']\n",
    "\n",
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "lyrics = df.lyrics.map(lambda lyric: normalize_lyric(lyric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 58768\n"
     ]
    }
   ],
   "source": [
    "lyrics_in_words = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.replace('\\n', ' \\n ').lower()\n",
    "    words = lyric.split(' ')\n",
    "    lyrics_in_words.append(words)\n",
    "    \n",
    "print('Corpus length in words:', len(list(itertools.chain(*lyrics_in_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 9013\n"
     ]
    }
   ],
   "source": [
    "words = set(list(itertools.chain(*lyrics_in_words)))\n",
    "print('Unique words:', len(words))\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "index_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 7\n",
    "step = 1\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "for lyric in lyrics_in_words:\n",
    "    for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "        sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "        next_words.append(lyric[i + maxlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datagenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, next_words_train, next_words_test = train_test_split(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "DIR = '../outputs/wordbased/LSTM_Simple_WordBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS, BATCH_SIZE, maxlen, step)\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(sentences_train, next_words_train, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "test_generator = DataGenerator(sentences_test, next_words_test, maxlen, word_index, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)\n",
    "#earlyStopping = EarlyStopping(patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(words))))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "680/680 [==============================] - 148s 218ms/step - loss: 6.4584 - val_loss: 6.0204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129cca438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "680/680 [==============================] - 200s 294ms/step - loss: 3.9257 - val_loss: 6.6072\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n",
      "wenn der benz anspringt und die reifen_\n"
     ]
    }
   ],
   "source": [
    "#AUTO TRAIN\n",
    "BATCH_SIZE = 64\n",
    "RANGE = 2\n",
    "GEN_WORD_LEN = 453\n",
    "training_generator = DataGenerator(sentences_train, next_words_train, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "test_generator = DataGenerator(sentences_test, next_words_test, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "for it in range(1, RANGE):\n",
    "    EPOCHS = 1\n",
    "    DIR = '../outputs/wordbased/LSTM_Simple_WordBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS*it, BATCH_SIZE, maxlen, step)\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "        \n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "    modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "    modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)\n",
    "\n",
    "    model.fit_generator(training_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])\n",
    "    \n",
    "    ##########\n",
    "    temperatures = [0.2, 0.4, 0.5, 0.6, 0.8, 1.]\n",
    "    for temperature in temperatures:\n",
    "        generated_text_temp = [\"wenn\", \"der\", \"benz\", \"anspringt\", \"und\", \"die\", \"reifen\"]\n",
    "        generated_text = copy(generated_text_temp)\n",
    "        print(\" \".join(generated_text) + '_')\n",
    "        #print('\\n___________________\\n')\n",
    "        for i in range(GEN_WORD_LEN):\n",
    "            sampled = np.zeros((1, maxlen, len(words)))\n",
    "\n",
    "            for t, word in enumerate(generated_text_temp):\n",
    "                sampled[0, t, word_index[word]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = index_word[next_index]\n",
    "            generated_text_temp.append(next_word)\n",
    "            generated_text.append(next_word)\n",
    "            generated_text_temp = generated_text_temp[1:]\n",
    "            #sys.stdout.write(\" \" + next_word)\n",
    "            \n",
    "        with open(os.path.join(DIR, '{}_temp{}_text.txt'.format(artists[0], temperature)), 'w+') as text_file:\n",
    "            text_file.write(' '.join(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wenn der benz anspringt und die reifen_\n",
      " tag \n",
      " ich bin ein - und du bist \n",
      " doch es ist noch immer wie ein mann \n",
      "  \n",
      "  \n",
      " kannst du die es sehen, kannst du gott haben mein mit hat \n",
      " in bleib mehr wenn der meine nicht ist \n",
      " mein rap ist geld und mein hund \n",
      " die es der nicht mehr einfach aus \n",
      " mein ohne ist mehr mit als weil es zum nicht mehr wie \n",
      " du für mir scheiße nie mehr von der - ich kannst du nicht mit mir auf sein \n",
      " du niemals mich einfach an der vater \n"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "random.seed(3004)\n",
    "\n",
    "#lyrics_index = random.randint(0, len(lyrics))\n",
    "#chosen_lyric = lyrics_in_words[lyrics_index]\n",
    "#start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "#generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text = [\"wenn\", \"der\", \"benz\", \"anspringt\", \"und\", \"die\", \"reifen\"]\n",
    "generated_text_temp = copy(generated_text_temp)\n",
    "print(\" \".join(generated_text) + '_')\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(100):\n",
    "    sampled = np.zeros((1, maxlen, len(words)))\n",
    "            \n",
    "    for t, word in enumerate(generated_text_temp):\n",
    "        sampled[0, t, word_index[word]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_word = index_word[next_index]\n",
    "    generated_text_temp.append(next_word)\n",
    "    generated_text.append(next_word)\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(\" \" + next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auf', 'dem', '\\n', 'ihr', '', '\\n', 'black,']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wenn',\n",
       " 'der',\n",
       " 'benz',\n",
       " 'anspringt',\n",
       " 'und',\n",
       " 'die',\n",
       " 'reifen',\n",
       " 'immer',\n",
       " 'als',\n",
       " 'ein',\n",
       " 'der',\n",
       " 'in',\n",
       " 'deinen',\n",
       " 'kein',\n",
       " 'im',\n",
       " '\\n',\n",
       " 'bist',\n",
       " 'weil',\n",
       " 'ein',\n",
       " 'mir',\n",
       " 'wenn',\n",
       " 'die',\n",
       " 'du',\n",
       " 'bist',\n",
       " 'ich',\n",
       " 'nicht',\n",
       " 'dich',\n",
       " 'a',\n",
       " 'als',\n",
       " '\\n',\n",
       " 'am',\n",
       " 'den',\n",
       " 'mir',\n",
       " 'am',\n",
       " 'was',\n",
       " 'ihr',\n",
       " 'dass',\n",
       " 'ihr',\n",
       " 'einfach',\n",
       " 'nicht',\n",
       " 'mal',\n",
       " 'und',\n",
       " 'die',\n",
       " 'dir',\n",
       " 'nicht',\n",
       " 'mehr',\n",
       " '\\n',\n",
       " 'ihr',\n",
       " 'habt',\n",
       " 'alle',\n",
       " \"'ner\",\n",
       " 'gegen',\n",
       " 'auf',\n",
       " '\\n',\n",
       " 'mann,',\n",
       " 'gibt',\n",
       " 'es',\n",
       " 'keine',\n",
       " 'wieder',\n",
       " 'war',\n",
       " '\\n',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'immer',\n",
       " 'noch',\n",
       " 'der',\n",
       " 'nicht',\n",
       " 'in',\n",
       " 'meinen',\n",
       " 'cool',\n",
       " '\\n',\n",
       " 'so',\n",
       " 'dein',\n",
       " 'ihr',\n",
       " 'leben',\n",
       " 'war',\n",
       " '\\n',\n",
       " 'ich',\n",
       " 'hab',\n",
       " \"'ne\",\n",
       " 'ein',\n",
       " 'mit',\n",
       " 'der',\n",
       " 'nie',\n",
       " 'mehr',\n",
       " 'nach',\n",
       " 'der',\n",
       " 'sein',\n",
       " '\\n',\n",
       " 'du',\n",
       " 'hast',\n",
       " 'ein',\n",
       " 'es',\n",
       " 'gibt',\n",
       " 'seine',\n",
       " \"'ne\",\n",
       " '\\n',\n",
       " 'wir',\n",
       " 'sind',\n",
       " 'ein',\n",
       " 'auf',\n",
       " 'dem',\n",
       " '\\n',\n",
       " 'ihr',\n",
       " '',\n",
       " '\\n',\n",
       " 'black,']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
