{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, CuDNNLSTM, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def normalize_lyric(text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub('\\[.+\\](\\\\n)|\\[.+\\](\\(.*\\))', '', text)\n",
    "    return text \n",
    "\n",
    "\n",
    "def plotTraining(model, epochs):\n",
    "        \"\"\"Plot graphs\"\"\"\n",
    "        # Load the training statistics (model.history)\n",
    "        # Plot training loss and accuracy\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        epochs = range(1, epochs + 1)\n",
    "        loss_values = model.history.history['loss']\n",
    "        val_loss_values = model.history.history['val_loss']        \n",
    "        \n",
    "        plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "        plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot validation loss and accuracy\n",
    "        # YOUR CODE HERE\n",
    "        acc_values = model.history.history['acc']\n",
    "        val_acc_values = model.history.history['val_acc']\n",
    "        \n",
    "        plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "        plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterts\n",
    "maxlen = 45  # extraxt sequences of n characters\n",
    "step = 3    # sample new seq every n characters\n",
    "n_grams_len = 0\n",
    "json_path = '../data/deutsch'\n",
    "artists = ['Bushido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Songs: 100\n",
      "Corpus length: 297389\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "lyrics = df.lyrics.map(lambda lyric: normalize_lyric(lyric))\n",
    "\n",
    "print('Number of Songs: {}'.format(len(df)))\n",
    "print('Corpus length: {}'.format(len(\"\".join(lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 97666\n",
      "Unique characters: 76\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "chars = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.lower()\n",
    "    if n_grams_len > 1:\n",
    "        for i in range(0, len(lyric) - maxlen - n_grams_len): # iterates by step size\n",
    "            sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "            next_chars.append(lyric[i + maxlen: i + maxlen + n_grams_len])\n",
    "        \n",
    "        ngrams_iter = ngrams(lyric, n_grams_len)\n",
    "        for gram in ngrams_iter:\n",
    "            chars.append(''.join(list(gram)))\n",
    "        chars = sorted(list(set(chars)))\n",
    "    else:\n",
    "        for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "            sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "            next_chars.append(lyric[i + maxlen])\n",
    "        \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "if n_grams_len < 1:\n",
    "    chars = sorted(list(set(''.join(lyrics)))) # list of unique characters\n",
    "\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) # maps char with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars))) # (sentences)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1    # one hot encoding\n",
    "    y[i, char_indices[next_chars[i]]] = 1  # one hot encoding\n",
    "    \n",
    "#for i, sentence in enumerate(sentences):\n",
    "#    for t in range(0, len(sentence) - n_grams_len):\n",
    "#        char = sentence[t:t+n_grams_len]\n",
    "#        x[i, t, char_indices[char]] = 1    # one hot encoding\n",
    "#    y[i, char_indices[next_chars[i]]] = 1  # one hot encodin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, next_chars_train, next_chars_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 45, 128)           105472    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 76)                9804      \n",
      "=================================================================\n",
      "Total params: 247,372\n",
      "Trainable params: 247,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DIR = '../outputs/charbased/2layer/LSTM_2Layer_CharBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS, BATCH_SIZE, maxlen, step)\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 219666 samples, validate on 73223 samples\n",
      "Epoch 1/1\n",
      "219666/219666 [==============================] - 152s 692us/step - loss: 1.5608 - val_loss: 1.5974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a415860>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences_train, next_chars_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(sentences_test, next_chars_test),\n",
    "          callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73249 samples, validate on 24417 samples\n",
      "Epoch 1/5\n",
      "73249/73249 [==============================] - 28s 382us/step - loss: 2.6646 - val_loss: 2.2863\n",
      "Epoch 2/5\n",
      "73249/73249 [==============================] - 28s 381us/step - loss: 2.2391 - val_loss: 2.0969\n",
      "Epoch 3/5\n",
      "73249/73249 [==============================] - 26s 361us/step - loss: 2.1017 - val_loss: 1.9928\n",
      "Epoch 4/5\n",
      "73249/73249 [==============================] - 28s 382us/step - loss: 2.0131 - val_loss: 1.9191\n",
      "Epoch 5/5\n",
      "73249/73249 [==============================] - 28s 379us/step - loss: 1.9430 - val_loss: 1.8676\n",
      "wenn der benz anspringt und die reifen wieder_\n",
      "wenn der benz anspringt und die reifen wieder_\n"
     ]
    }
   ],
   "source": [
    "for it in range(1, 10):\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 64\n",
    "    GEN_CHAR_LEN = 2973\n",
    "\n",
    "    DIR = '../outputs/charbased/2layer_dropout_lr0.001/LSTM_2Layer_lr0.001_CharBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS*it, BATCH_SIZE, maxlen, step)\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "    modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "    modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)\n",
    "        \n",
    "    model.fit(sentences_train, next_chars_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(sentences_test, next_chars_test),\n",
    "          callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])\n",
    "    \n",
    "    temperatures = [0.2, 0.4, 0.5, 0.6, 0.8, 1.]\n",
    "    for temperature in temperatures:\n",
    "        generated_text_temp = \"wenn der benz anspringt und die reifen wieder\"\n",
    "        generated_text = generated_text_temp\n",
    "        print(generated_text + '_')\n",
    "        #print('\\n___________________\\n')\n",
    "        for i in range(GEN_CHAR_LEN):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "\n",
    "            for t, char in enumerate(generated_text_temp):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            generated_text_temp += next_char\n",
    "            generated_text += next_char\n",
    "            generated_text_temp = generated_text_temp[1:]\n",
    "            #sys.stdout.write(next_char)\n",
    "        \n",
    "        with open(os.path.join(DIR, '{}_temp{}_text.txt'.format(artists[0], temperature)), 'w+') as text_file:\n",
    "            text_file.write(generated_text)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wenn der benz anspringt und die reifen wieder_\n",
      " herz\n",
      "und der ich hab' mich schein freunde wie deinen teil die mama sieht\n",
      "ich bin der himmel mir auf deine mutter peine musste\n",
      "hängt man kuter keine streit, dass die schickt\n",
      "scheiß fallende resten treiber\n",
      "ich bin ein platz, der bruder die ganze scheine\n",
      "\n",
      "das ist mein hunde hat der aller, der für mich\n",
      "ich bin euer die ganze redienste reinen\n",
      "dass die schlasse berliner hat ihr will mich aus\n",
      "die scheine welter sich wie ein arsch\n",
      "das ist die schlicht auf die schwarz es ein erdesenter\n",
      "habe meine scheine ersticht haben, dass ich will\n",
      "wir die schickte mich sagt mal hier jetzt bist\n",
      "war die kingt schlieft und das ist schwarzgelden und im schwanz sein\n",
      "ich bin deine tränen scheine nicht mehr eine erzähle\n",
      "der verschwinne ist meine straße und das bist\n",
      "du warst nur bin es leiden hat die schralen zu lebt\n",
      "ich bin der leben, war die wesst du deine schweine sohne ein sohne schein, dass ich sein in dem fallen\n",
      "fick mich nur in der haufen bist du verliehst\n",
      "habe die leben alles mit deiner hand\n",
      "ich scheiß dich so wie ein pulste deine bushidot\n",
      "denn kannst es geht im fallout!\n",
      "der nacht wenn ich mich ist am harten bara\n",
      "freiseht nach mit dieser aller\n",
      "das will du willst ein puterscheine warschen ist nichts\n",
      "dann versteht ist die händen bist du wisse hier geht ist die scheint\n",
      "du lasss der fallout, schwiefes liegen nur das leben\n",
      "hier bist du bist ein anzen freunde hat mich schein reis wie du will\n",
      "man gewollt mich an die schafft, dass man hon'ne gehst\n",
      "du bist mir will mich ist meine freunde auf mich\n",
      "ich bin d"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "\n",
    "#start_index = random.randint(0, len(lyrics) - maxlen - 1)\n",
    "#generated_text = lyrics[start_index: start_index + maxlen]\n",
    "#random.seed(3004)\n",
    "#lyrics_index = random.randint(0, len(lyrics))\n",
    "#chosen_lyric = lyrics[lyrics_index]\n",
    "#start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "#generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text_temp = \"wenn der benz anspringt und die reifen wieder\"\n",
    "generated_text = generated_text_temp\n",
    "print(generated_text + '_')\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(1500):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            \n",
    "    for t, char in enumerate(generated_text_temp):\n",
    "        sampled[0, t, char_indices[char]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = chars[next_index]\n",
    "    generated_text_temp += next_char\n",
    "    generated_text += next_char\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"wenn der benz anspringt und die reifen wieder sein\\nich kann den kommen in den schaffen für dich\\nich bin der schwuchel dieser andere schleiße\\ndenn wenn der heiman, ich weiß du wieder scheiße\\ndenn wenn der heiman, ich war die schreie dich\\ndenn wir kommen, wie du den schwule schlafen ist schleiß auf den schutz\\nes ist so wie gern kann ich weiß sein ich wie ein grauen schlafe\\ndenn ich keine feinde in den schatz, ich schaff\\nich weiß du keine feinde auf dem schwanzen geschicht\\ndenn wenn der hart nicht schon mit den kommt schleinte\\nich schaffe dich den schwule sein schleiß stadt\\ndenn ich weiß du keine schleinen hart sehen\\ndenn wir kommen, wie das gesehen, dass du ein freunde\\ndenn ich weiß sein erwegen kommt der stadt\\nich hab' die schwanz, wie du den schwanzen schickt\\ndas ist der kommen geld macht den schlechten an die schatz, was ich den schwarzen auf dem komm\\nich hab die schwanzen auf der wollen schule\\nes ist euch scheiß den aller schickt\\nich bin der stadt an die schwuchte sich schwanz\\nich hab' dir deine mutter kommen geld schieft\\ndenn ich schaffe dieser scheiße wie ein freunde\\nich bin der schwer in den schwanz schatz mit den schwanz\\nich bin der stadt, ich war dich an die schwanz\\nich schaffe dich in den schaffen wie ein freunde\\ndenn ich bin der traum schleine wie ein freunde\\ndenn wenn die schaff, dass du auf den schaffen schlafe\\ndenn wenn der beat nicht mehr gelaunt auf der schwer schaff\\nich kann dich den schwanzen wie ein schwanz schatz\\ndenn ich will dich kein bleibt dich mit den scheiße\\ndenn ich will dich auf der stadt\\n\\nich wollte dich keine bei den schwule der schwanz sein\\nich weiß sie scheiße schaffen in den schatz\\nich war dich den stehen weiß ich die schule\\ndas ist der falletent scheiße in den schlechten\\ndas ist schwarzen schaffen im scheiß gesellster\\ndenn ich mich den schweine auf der schwanz sich\\nich bin den stadt, wie das gesellstem in den schaffe\\ndenn ich war dich auf den schwule sein ich den schwarzen\\nich bereiß dich schaffen auf der schweine mit den scheiße\\ndenn wenn der freunde und den stadt die schleiße\\ndenn wir sind die scheiße schule, dass du keine stadt\\ndenn ich bin der treuer die schule schaffen wie ein bordstein\\nich war die schlechten hart im schatz, denn ich machen\\nich kann dich schleiß die schaff, ich kann die schutzen\\ndas ist der hutter, wenn die kam in den schon mit den schwule den schaff\\ndenn du hast steh'n scheiße scheiße schaffen schaffen\\nich schaffe dich so wie dich den schaffen schlafe\\nich sein mir nicht mehr voll die schule schaff\\nich baller dich an die schaff, ich weiß\\ndu schleiger schaffen hart ist die schwarzen schaffen\\ndu bist an die schaffen schaffen ich gesele\\ndenn wenn die aller, wenn die schaff, dass du die schreien\\ndu bist in den schwanz auf der stadt\\n\\nschon wie ein schaff, ich bin der stadt\\ndenn ich war die schaff, dann dich nicht mehr wie ein freunde\\ndenn wenn der stadt deine schwule stehst du gesellst\\nich will dich harten sich wie ein freunde wie ein bordsteine\\ndenn ich kein gesellschaffen in den schule die scheiße\\ndenn wenn der fr\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../outputs/charbased/LSTM_Simple_CharBased_Bushido_E1_BS64_ML45_SS1/temp0.5_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4c8273d608a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#file_name = \"Bushido_60epochs_30maxlen_0ngrams\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp{}_text.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../outputs/charbased/LSTM_Simple_CharBased_Bushido_E1_BS64_ML45_SS1/temp0.5_text.txt'"
     ]
    }
   ],
   "source": [
    "#file_name = \"Bushido_60epochs_30maxlen_0ngrams\"\n",
    "with open(os.path.join(DIR, 'temp{}_text.txt'.format(temperature))) as text_file:\n",
    "    text_file.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
