{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../../data/deutsch'\n",
    "artists = ['Bushido']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "print(len(df))\n",
    "lyrics = df.lyrics.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 60292\n"
     ]
    }
   ],
   "source": [
    "lyrics_in_words = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.replace('\\n', ' \\n ').lower()\n",
    "    words = lyric.split(' ')\n",
    "    lyrics_in_words.append(words)\n",
    "    \n",
    "print('Corpus length in words:', len(list(itertools.chain(*lyrics_in_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 9108\n"
     ]
    }
   ],
   "source": [
    "words = set(list(itertools.chain(*lyrics_in_words)))\n",
    "print('Unique words:', len(words))\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "index_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 10  # extraxt sequences of n characters\n",
    "step = 1\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "for lyric in lyrics_in_words:\n",
    "    for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "        sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "        next_words.append(lyric[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, next_words_train, next_words_test = train_test_split(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, sentences, next_words, maxlen, word_index, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.next_words = next_words\n",
    "        self.sentences = sentences\n",
    "        self.shuffle = shuffle\n",
    "        self.maxlen = maxlen\n",
    "        self.word_index = word_index\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        sentences_temp = [self.sentences[k] for k in indexes]\n",
    "        next_words_temp = [self.next_words[l] for l in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(sentences_temp, next_words_temp)\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, sentences, next_words):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((len(sentences), self.maxlen, len(word_index))) # (sentences)\n",
    "        y = np.zeros((len(sentences), len(word_index)), dtype=np.bool)\n",
    "\n",
    "        # Generate data\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, word in enumerate(sentence):\n",
    "                X[i, t, word_index[word]] = 1    # one hot encoding\n",
    "                y[i, word_index[next_words[i]]] = 1\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "DIR = './wordbased/WordBased_{}_E{}'.format(artists[0], EPOCHS)\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(sentences_train, next_words_train, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "test_generator = DataGenerator(sentences_test, next_words_test, maxlen, word_index, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'))\n",
    "modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"))\n",
    "#earlyStopping = EarlyStopping(patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128), input_shape=(maxlen, len(words))))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "694/694 [==============================] - 481s 693ms/step - loss: 6.3129 - val_loss: 5.8551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12af4c080>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[tensorboard, modelCheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seht es ein, ihr werdet nie so leben \n",
      " es_\n",
      " ist nur ein yeah, \n",
      " ich bin der wär und ich bin \n",
      " ich bin dir dir in der selbst \n",
      " du bist die jetzt war nicht mehr an \n",
      "  \n",
      " [hook] \n",
      " ich hab in der [part war noch ich \n",
      " ich hab die ganze freunde drauf war \n",
      " ich bin der 'nem bastard \n",
      " ich bin dir kommt ich die  \n",
      "  \n",
      " [hook] \n",
      " es ist nur immer noch der hart \n",
      " ich bin den produziere für die in der straße \n",
      " doch ich nicht aus mehr deiner keinen sagt, \n",
      "  \n",
      " [hook] \n"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "random.seed(3004)\n",
    "\n",
    "lyrics_index = random.randint(0, len(lyrics))\n",
    "chosen_lyric = lyrics_in_words[lyrics_index]\n",
    "start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text = copy(generated_text_temp)\n",
    "print(\" \".join(generated_text) + '_')\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(100):\n",
    "    sampled = np.zeros((1, maxlen, len(words)))\n",
    "            \n",
    "    for t, word in enumerate(generated_text_temp):\n",
    "        sampled[0, t, word_index[word]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_word = index_word[next_index]\n",
    "    generated_text_temp.append(next_word)\n",
    "    generated_text.append(next_word)\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(\" \" + next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
