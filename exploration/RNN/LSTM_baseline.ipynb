{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterts\n",
    "maxlen = 60  # extraxt sequences of n characters\n",
    "step = 3     # sample new seq every n characters\n",
    "n_grams_len = 0\n",
    "json_path = '../../data/deutsch'\n",
    "artists = ['Bushido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Songs: 100\n",
      "Corpus length: 303654\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "lyrics = df.lyrics.map(lambda lyric: lyric.lower())\n",
    "\n",
    "print('Number of Songs: {}'.format(len(df)))\n",
    "print('Corpus length: {}'.format(len(\"\".join(lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 99246\n",
      "Unique characters: 78\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "chars = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.lower()\n",
    "    if n_grams_len > 1:\n",
    "        for i in range(0, len(lyric) - maxlen - n_grams_len): # iterates by step size\n",
    "            sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "            next_chars.append(lyric[i + maxlen: i + maxlen + n_grams_len])\n",
    "        \n",
    "        ngrams_iter = ngrams(lyric, n_grams_len)\n",
    "        for gram in ngrams_iter:\n",
    "            chars.append(''.join(list(gram)))\n",
    "        chars = sorted(list(set(chars)))\n",
    "    else:\n",
    "        for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "            sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "            next_chars.append(lyric[i + maxlen])\n",
    "        \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "if n_grams_len < 1:\n",
    "    chars = sorted(list(set(''.join(lyrics)))) # list of unique characters\n",
    "\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) # maps char with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9563"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i, sentence in enumerate(sentences):\\n    for t in range(0, len(sentence) - n_grams_len):\\n        char = sentence[t:t+n_grams_len]\\n        x[i, t, char_indices[char]] = 1    # one hot encoding\\n    y[i, char_indices[next_chars[i]]] = 1  # one hot encoding'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars))) # (sentences)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1    # one hot encoding\n",
    "    y[i, char_indices[next_chars[i]]] = 1  # one hot encoding\n",
    "    \n",
    "\"\"\"\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t in range(0, len(sentence) - n_grams_len):\n",
    "        char = sentence[t:t+n_grams_len]\n",
    "        x[i, t, char_indices[char]] = 1    # one hot encoding\n",
    "    y[i, char_indices[next_chars[i]]] = 1  # one hot encodin\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               105984    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 78)                10062     \n",
      "=================================================================\n",
      "Total params: 116,046\n",
      "Trainable params: 116,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "99246/99246 [==============================] - 104s 1ms/step - loss: 0.9756\n",
      "Epoch 2/60\n",
      "99246/99246 [==============================] - 103s 1ms/step - loss: 0.9754\n",
      "Epoch 3/60\n",
      "99246/99246 [==============================] - 104s 1ms/step - loss: 0.9727\n",
      "Epoch 4/60\n",
      "99246/99246 [==============================] - 104s 1ms/step - loss: 0.9738\n",
      "Epoch 5/60\n",
      "99246/99246 [==============================] - 103s 1ms/step - loss: 0.9771\n",
      "Epoch 6/60\n",
      "99246/99246 [==============================] - 100s 1ms/step - loss: 0.9683\n",
      "Epoch 7/60\n",
      "99246/99246 [==============================] - 59s 595us/step - loss: 0.9760\n",
      "Epoch 8/60\n",
      "99246/99246 [==============================] - 56s 569us/step - loss: 0.9713\n",
      "Epoch 9/60\n",
      "99246/99246 [==============================] - 58s 583us/step - loss: 0.9689\n",
      "Epoch 10/60\n",
      "99246/99246 [==============================] - 57s 575us/step - loss: 0.9627\n",
      "Epoch 11/60\n",
      "99246/99246 [==============================] - 57s 571us/step - loss: 0.9729\n",
      "Epoch 12/60\n",
      "99246/99246 [==============================] - 57s 577us/step - loss: 0.9688\n",
      "Epoch 13/60\n",
      "99246/99246 [==============================] - 58s 584us/step - loss: 0.9638\n",
      "Epoch 14/60\n",
      "99246/99246 [==============================] - 57s 571us/step - loss: 0.9602\n",
      "Epoch 15/60\n",
      "99246/99246 [==============================] - 58s 581us/step - loss: 0.9560\n",
      "Epoch 16/60\n",
      "99246/99246 [==============================] - 60s 607us/step - loss: 0.9619\n",
      "Epoch 17/60\n",
      "99246/99246 [==============================] - 111s 1ms/step - loss: 0.9548\n",
      "Epoch 18/60\n",
      "99246/99246 [==============================] - 112s 1ms/step - loss: 0.9582\n",
      "Epoch 19/60\n",
      "99246/99246 [==============================] - 112s 1ms/step - loss: 0.9596\n",
      "Epoch 20/60\n",
      "99246/99246 [==============================] - 112s 1ms/step - loss: 0.9566\n",
      "Epoch 21/60\n",
      "99246/99246 [==============================] - 73s 738us/step - loss: 0.9627\n",
      "Epoch 22/60\n",
      "  896/99246 [..............................] - ETA: 57s - loss: 0.8245"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "model.fit(x, y, batch_size=128, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = artists[0]\n",
    "epochs = 180\n",
    "file_name = '{}_{}epochs_{}maxlen'.format(artist, epochs, maxlen, n_grams_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bushido_120epochs_60maxlen'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hao/workspace/hpi-de/dl4textmining/dl4tm-project/exploration/RNN'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./increase_epochs/models/model_{}.h5'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('./model_Bushido_60epochs_30maxlen_0ngrams.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mach den fernseher an, kleine bushidos\n",
      "ich kam aus dem ghett\n",
      "o seh wie ich mehr die greeben\n",
      "dein erziehten frieden schwein in den schönel freshhlingt\n",
      "passinaut gestatt, yeah\n",
      "\n",
      "[part 2]\n",
      "das ich mache, wer "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hao/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich bin ich musste dich gebust\n",
      "die gedamper sind wie ich dich meine stadt, was renze weiter\n",
      "ich bin deine mutter machst du die geschichten und dann berlin\n",
      "ich hab mach ich will ich mir alles keiner so vaner\n",
      "ihr könnte von deinem eisen\n",
      "du bist um benz am kurschau mein leben\n",
      "und ich schaffen grauerapart\n",
      "ich seh mich glaubt und alles sehen werden wie in, bapal, weil du kein'n choruser und den stell in der schlechten tag bst\n",
      "ich schaff es nich jetzt mein mir an\n",
      "\n",
      "[part 2]\n",
      "deine freunde würde dich?, frenn\n",
      "sag misss den staatsaus\n",
      "wir für die ganzen leinkie hängen ich machs' die scheißen wenn der ferren\n",
      "meine schreißt mir ob ich keine skalt meine schrechten zieht\n",
      "wer ein allo und schon so das du bege einer leben\n",
      "ich war die schlechten zeiten und auch die gesucht war nur bei nicht\n",
      "die shindy mir alles fehlen\n",
      "man sich wenn ich machen, dass man auf der stell in der juice du siehst\n",
      "\n",
      "[hook]\n",
      "ich hab gewächte alter gebleit verlier?\n",
      "denn sie woll' nur erst du alles der bollys, wenn du sie wirst du foriert ihn geschafft\n",
      "in der bester aus dem benz amg, mit seinem einer bei nacht aus\n",
      "\n",
      "[hook]\n",
      "ich hab glaub mir\n",
      "ich schaff' ich alleine zeig!\n",
      "ich hab den strelt, weil du west ich hatte\n",
      "deine machten wie?y kay“s\n",
      "werli':biert ist ein scheine mann\n",
      "für dich ich auch euch fotzen\n",
      "um boss am kullloo\n",
      "jemen wie du allen sein'n kam der beats haifn\n",
      "du hast mich gehst\n",
      "ich"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "\n",
    "#start_index = random.randint(0, len(lyrics) - maxlen - 1)\n",
    "#generated_text = lyrics[start_index: start_index + maxlen]\n",
    "random.seed(3004)\n",
    "lyrics_index = random.randint(0, len(lyrics))\n",
    "chosen_lyric = lyrics[lyrics_index]\n",
    "start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text = generated_text_temp\n",
    "print(generated_text)\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(1500):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            \n",
    "    for t, char in enumerate(generated_text_temp):\n",
    "        sampled[0, t, char_indices[char]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = chars[next_index]\n",
    "    generated_text_temp += next_char\n",
    "    generated_text += next_char\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"Bushido_60epochs_30maxlen_0ngrams\"\n",
    "with open('./increase_epochs/texts/' + file_name + '.txt', 'w') as text_file:\n",
    "    text_file.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
