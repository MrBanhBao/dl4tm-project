{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPool1D, AveragePooling1D, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, sentences, next_words, maxlen, word_index, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.next_words = next_words\n",
    "        self.sentences = sentences\n",
    "        self.shuffle = shuffle\n",
    "        self.maxlen = maxlen\n",
    "        self.word_index = word_index\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        sentences_temp = [self.sentences[k] for k in indexes]\n",
    "        next_words_temp = [self.next_words[l] for l in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(sentences_temp, next_words_temp)\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, sentences, next_words):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((len(sentences), self.maxlen, len(word_index))) # (sentences)\n",
    "        y = np.zeros((len(sentences), len(word_index)), dtype=np.bool)\n",
    "\n",
    "        # Generate data\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, word in enumerate(sentence):\n",
    "                X[i, t, word_index[word]] = 1    # one hot encoding\n",
    "                y[i, word_index[next_words[i]]] = 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def normalize_lyric(text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub('\\[.+\\](\\\\n)|\\[.+\\](\\(.*\\))', '', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../data/deutsch'\n",
    "artists = ['Bushido']\n",
    "\n",
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "lyrics = df.lyrics.map(lambda lyric: normalize_lyric(lyric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 58768\n"
     ]
    }
   ],
   "source": [
    "lyrics_in_words = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.replace('\\n', ' \\n ').lower()\n",
    "    words = lyric.split(' ')\n",
    "    lyrics_in_words.append(words)\n",
    "    \n",
    "print('Corpus length in words:', len(list(itertools.chain(*lyrics_in_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 9013\n"
     ]
    }
   ],
   "source": [
    "words = set(list(itertools.chain(*lyrics_in_words)))\n",
    "print('Unique words:', len(words))\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "index_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 7\n",
    "step = 1\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "for lyric in lyrics_in_words:\n",
    "    for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "        sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "        next_words.append(lyric[i + maxlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datagenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, next_words_train, next_words_test = train_test_split(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "DIR = '../outputs/wordbased/CNN_Simple_WordBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS, BATCH_SIZE, maxlen, step)\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(sentences_train, next_words_train, maxlen, word_index, batch_size=BATCH_SIZE)\n",
    "test_generator = DataGenerator(sentences_test, next_words_test, maxlen, word_index, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)\n",
    "#earlyStopping = EarlyStopping(patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(input_shape=(maxlen, len(words)),\n",
    "                filters=32,\n",
    "                kernel_size=7,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64,\n",
    "                kernel_size=3,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "#model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "\"\"\"model.add(Conv1D(filters=64,\n",
    "                kernel_size=3,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32,\n",
    "                kernel_size=3,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\"\"\"\n",
    "model.add(AveragePooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "680/680 [==============================] - 79s 116ms/step - loss: 6.7232 - val_loss: 6.5135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e479748>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anderen bushido geben \n",
      "  \n",
      " ich_\n",
      " ich der \n",
      " \n",
      " \n",
      " du hier ein der auch \n",
      " ich \n",
      " der kein \n",
      " \n",
      " und du die \n",
      " \n",
      " die wie \n",
      " ich \n",
      " wer \n",
      " \n",
      " ich bin wieder du \n",
      " \n",
      " ich du als \n",
      " \n",
      " nicht ich der dir \n",
      " \n",
      " ich nie \n",
      " ich \n",
      " \n",
      " sie wie nicht \n",
      " ich kannst du du \n",
      " ich \n",
      " ich du \n",
      " ich der ein hab \n",
      " ist \n",
      " ich der \n",
      " \n",
      " ich ich deinen du die warst \n",
      " ich \n",
      " du die \n",
      " sonny der \n",
      " wie ist \n",
      " ich es gibt \n"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "random.seed(3004)\n",
    "\n",
    "lyrics_index = random.randint(0, len(lyrics))\n",
    "chosen_lyric = lyrics_in_words[lyrics_index]\n",
    "start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text = copy(generated_text_temp)\n",
    "print(\" \".join(generated_text) + '_')\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(100):\n",
    "    sampled = np.zeros((1, maxlen, len(words)))\n",
    "            \n",
    "    for t, word in enumerate(generated_text_temp):\n",
    "        sampled[0, t, word_index[word]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_word = index_word[next_index]\n",
    "    generated_text_temp.append(next_word)\n",
    "    generated_text.append(next_word)\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(\" \" + next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
