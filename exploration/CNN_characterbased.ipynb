{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from pandas.io.json import json_normalize\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPool1D, AveragePooling1D, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path, artists=[]):\n",
    "    if (os.path.isfile(json_path)):\n",
    "        print(\"json\")\n",
    "        with open(json_path) as f:\n",
    "            song_data = json.load(f)\n",
    "            return song_data['songs']\n",
    "        \n",
    "    elif (os.path.isdir(json_path)):\n",
    "        data = []\n",
    "        json_files = []\n",
    "        if (len(artists) > 0):\n",
    "            for artist in artists:\n",
    "                json_files = json_files + [json_file for json_file in os.listdir(json_path) if ((json_file.endswith('.json')) & (artist in json_file))]\n",
    "        else:\n",
    "            json_files = [json_file for json_file in os.listdir(json_path) if json_file.endswith('.json')]\n",
    "\n",
    "        for json_file in json_files:\n",
    "            path_to_json = os.path.join(json_path, json_file)\n",
    "            with open(path_to_json) as f:\n",
    "                song_data = json.load(f)\n",
    "                data = data + song_data['songs']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    \n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def normalize_lyric(text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub('\\[.+\\](\\\\n)|\\[.+\\](\\(.*\\))', '', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterts\n",
    "maxlen = 45  # extraxt sequences of n characters\n",
    "step = 1    # sample new seq every n characters\n",
    "n_grams_len = 0\n",
    "json_path = '../data/deutsch'\n",
    "artists = ['Bushido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Songs: 100\n",
      "Corpus length: 297389\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = load_json(json_path, artists)\n",
    "df = json_normalize(data)\n",
    "lyrics = df.lyrics.map(lambda lyric: normalize_lyric(lyric))\n",
    "\n",
    "print('Number of Songs: {}'.format(len(df)))\n",
    "print('Corpus length: {}'.format(len(\"\".join(lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 292889\n",
      "Unique characters: 76\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "chars = []\n",
    "for lyric in lyrics:\n",
    "    lyric = lyric.lower()\n",
    "    if n_grams_len > 1:\n",
    "        for i in range(0, len(lyric) - maxlen - n_grams_len): # iterates by step size\n",
    "            sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "            next_chars.append(lyric[i + maxlen: i + maxlen + n_grams_len])\n",
    "        \n",
    "        ngrams_iter = ngrams(lyric, n_grams_len)\n",
    "        for gram in ngrams_iter:\n",
    "            chars.append(''.join(list(gram)))\n",
    "        chars = sorted(list(set(chars)))\n",
    "    else:\n",
    "        for i in range(0, len(lyric) - maxlen, step): # iterates by step size\n",
    "            sentences.append(lyric[i: i + maxlen]) # get maxlen amount of characters\n",
    "            next_chars.append(lyric[i + maxlen])\n",
    "        \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "if n_grams_len < 1:\n",
    "    chars = sorted(list(set(''.join(lyrics)))) # list of unique characters\n",
    "\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) # maps char with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars))) # (sentences)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1    # one hot encoding\n",
    "    y[i, char_indices[next_chars[i]]] = 1  # one hot encoding\n",
    "    \n",
    "#for i, sentence in enumerate(sentences):\n",
    "#    for t in range(0, len(sentence) - n_grams_len):\n",
    "#        char = sentence[t:t+n_grams_len]\n",
    "#        x[i, t, char_indices[char]] = 1    # one hot encoding\n",
    "#    y[i, char_indices[next_chars[i]]] = 1  # one hot encodin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, next_chars_train, next_chars_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(input_shape=(maxlen, len(chars)),\n",
    "                filters=32,\n",
    "                kernel_size=7,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64,\n",
    "                kernel_size=3,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "#model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "\"\"\"model.add(Conv1D(filters=64,\n",
    "                kernel_size=3,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32,\n",
    "                kernel_size=3,\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                strides=1))\"\"\"\n",
    "model.add(AveragePooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 45, 32)            17056     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 22, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 22, 64)            6208      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 704)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 76)                53580     \n",
      "=================================================================\n",
      "Total params: 76,844\n",
      "Trainable params: 76,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DIR = '../outputs/charbased/CNN_Simple_CharBased_{}_E{}_BS{}_ML{}_SS{}'.format(artists[0], EPOCHS, BATCH_SIZE, maxlen, step)\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=os.path.join(DIR, 'logs'), write_images=True, write_grads=True)\n",
    "modelCheckpoint_best = ModelCheckpoint(filepath=os.path.join(DIR, \"model_best.h5\"), save_best_only=True)\n",
    "modelCheckpoint = ModelCheckpoint(filepath=os.path.join(DIR, \"model.h5\"), save_best_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 219666 samples, validate on 73223 samples\n",
      "Epoch 1/1\n",
      "219666/219666 [==============================] - 33s 148us/step - loss: 2.2044 - val_loss: 2.1261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1082501d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences_train, next_chars_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(sentences_test, next_chars_test),\n",
    "          callbacks=[tensorboard, modelCheckpoint, modelCheckpoint_best])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shidos\n",
      "ich kam aus dem ghetto auf die leinwan\n",
      "n ich wie kan und die denn in die kicken  zum ferdun mich hericht junge sein \n",
      "dick senn ist denn sos, ich haben die mir was wir dich bin hier rinzast wein ich deine seine nin die fickt fass ip, sein tlog perbe mein so auf\n",
      "die schah ainter kinden mein rib ein clip gesten es die mein und rasen\n",
      "wie mergenicht ich beiwe und der dich auf die keine deinen fannst du die denn die e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hao/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ins jeden mir in die dich ist und der verim sind mir in vorser anden nicht\n",
      "der du ich das yaft\n",
      "ich wir dich dich sein nicht die big wie bar \n",
      "ich bin manne geinmene bann manne tifst du jeld blare einen blann dein ein zu wenn ihr schann wir wie big in und ich ich mir schund ich woll die dich die des cizzum habt wit wes nicht denn fann auf die für dich sind heur astnicht wir eine inder mir deinen eint mir richt mir die auf der beifen jeld die bin so pein, gannst du nenndas nicht die zu mich ist erafwec gingzwirne anden was ist dich nicht die gonnk f win dar ticht sobe will kann ich die ist du willst ern in schaln soingann die denn denn jeld sind den die woin die verwicht einen  dise gein boir\n",
      "ich bin sond ich bin mich enden rauf den einginnter -wein der wie ind und schusste kalod ein mir auf der dich die lann war was war genden glar ist du dannen weine zu siehs ainst du schace weine sind wird mitser zinds wir mir wer ich sich sein are songst du nicht die dich will du nicht die fim-tre schussten jetzt weitern für die die jaffurim anngackkers nicht die frick in fehr mir geht die kamf viele geigim bos denn die wo"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "\n",
    "#start_index = random.randint(0, len(lyrics) - maxlen - 1)\n",
    "#generated_text = lyrics[start_index: start_index + maxlen]\n",
    "random.seed(3004)\n",
    "lyrics_index = random.randint(0, len(lyrics))\n",
    "chosen_lyric = lyrics[lyrics_index]\n",
    "start_index = random.randint(0, len(chosen_lyric) - maxlen - 1)\n",
    "generated_text_temp = chosen_lyric[start_index: start_index + maxlen]\n",
    "generated_text = generated_text_temp\n",
    "print(generated_text)\n",
    "#print('\\n___________________\\n')\n",
    "for i in range(1500):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            \n",
    "    for t, char in enumerate(generated_text_temp):\n",
    "        sampled[0, t, char_indices[char]] = 1.\n",
    "                      \n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = chars[next_index]\n",
    "    generated_text_temp += next_char\n",
    "    generated_text += next_char\n",
    "    generated_text_temp = generated_text_temp[1:]\n",
    "    sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"Bushido_60epochs_30maxlen_0ngrams\"\n",
    "with open('./increase_epochs/texts/' + file_name + '.txt', 'w') as text_file:\n",
    "    text_file.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
